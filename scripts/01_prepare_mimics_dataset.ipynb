{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook converts the csv files to pickle files for quick processing in python.\n",
    "Prerequisites: all the csv files of the mimics-iii database is dumped into a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/pytz/__init__.py:31: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  match = re.match(\"^#\\s*version\\s*([0-9a-z]*)\\s*$\", line)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/asusaiyah/data/tactics_storage/mimic-iii/\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import os,sys\n",
    "import pandas as pd\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "folder_containing_mimics_csv_files = os.environ.get(\"MIMICPATH\")\n",
    "print(folder_containing_mimics_csv_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(os.environ.get(\"MIMICSPATH\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/asusaiyah/data/tactics_storage/mimic-iii/CAREGIVERS.csv', '/home/asusaiyah/data/tactics_storage/mimic-iii/PATIENTS.csv', '/home/asusaiyah/data/tactics_storage/mimic-iii/SERVICES.csv', '/home/asusaiyah/data/tactics_storage/mimic-iii/PROCEDURES_ICD.csv', '/home/asusaiyah/data/tactics_storage/mimic-iii/ADMISSIONS.csv', '/home/asusaiyah/data/tactics_storage/mimic-iii/TRANSFERS.csv', '/home/asusaiyah/data/tactics_storage/mimic-iii/PROCEDUREEVENTS_MV.csv', '/home/asusaiyah/data/tactics_storage/mimic-iii/MICROBIOLOGYEVENTS.csv', '/home/asusaiyah/data/tactics_storage/mimic-iii/OUTPUTEVENTS.csv', '/home/asusaiyah/data/tactics_storage/mimic-iii/PRESCRIPTIONS.csv', '/home/asusaiyah/data/tactics_storage/mimic-iii/INPUTEVENTS_MV.csv', '/home/asusaiyah/data/tactics_storage/mimic-iii/LABEVENTS.csv', '/home/asusaiyah/data/tactics_storage/mimic-iii/INPUTEVENTS_CV.csv', '/home/asusaiyah/data/tactics_storage/mimic-iii/NOTEEVENTS.csv']\n",
      "Processing /home/asusaiyah/data/tactics_storage/mimic-iii/CAREGIVERS.csv...\n",
      "Processing /home/asusaiyah/data/tactics_storage/mimic-iii/PATIENTS.csv...\n",
      "Processing /home/asusaiyah/data/tactics_storage/mimic-iii/SERVICES.csv...\n",
      "Processing /home/asusaiyah/data/tactics_storage/mimic-iii/PROCEDURES_ICD.csv...\n",
      "Processing /home/asusaiyah/data/tactics_storage/mimic-iii/ADMISSIONS.csv...\n",
      "Processing /home/asusaiyah/data/tactics_storage/mimic-iii/TRANSFERS.csv...\n",
      "Processing /home/asusaiyah/data/tactics_storage/mimic-iii/PROCEDUREEVENTS_MV.csv...\n",
      "Processing /home/asusaiyah/data/tactics_storage/mimic-iii/MICROBIOLOGYEVENTS.csv...\n",
      "Processing /home/asusaiyah/data/tactics_storage/mimic-iii/OUTPUTEVENTS.csv...\n",
      "Processing /home/asusaiyah/data/tactics_storage/mimic-iii/PRESCRIPTIONS.csv...\n",
      "Processing /home/asusaiyah/data/tactics_storage/mimic-iii/INPUTEVENTS_MV.csv...\n",
      "Processing /home/asusaiyah/data/tactics_storage/mimic-iii/LABEVENTS.csv...\n",
      "Processing /home/asusaiyah/data/tactics_storage/mimic-iii/INPUTEVENTS_CV.csv...\n",
      "Processing /home/asusaiyah/data/tactics_storage/mimic-iii/NOTEEVENTS.csv...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filelist = sorted(glob(f\"{folder_containing_mimics_csv_files}/*.csv\"), key=os.path.getsize)\n",
    "print(filelist)\n",
    "for file_path in filelist:\n",
    "    os.makedirs(\"../data\",exist_ok=True)\n",
    "    op_file = os.path.join(\"../data/\", os.path.basename(file_path).replace(\".csv\", \".pkl\"))\n",
    "    print(f\"Processing {file_path}...\")\n",
    "    if os.path.exists(op_file):\n",
    "        print(f\"Skipping {file_path}, already processed.\")\n",
    "        continue\n",
    "    if \"ADMISSIONS.csv\" in file_path:\n",
    "        parse_cols =  [\"ADMITTIME\", \"DISCHTIME\"]\n",
    "    else:\n",
    "        parse_cols = None\n",
    "        \n",
    "    chunks = pd.read_csv(file_path, parse_dates=parse_cols, chunksize=10000)\n",
    "    all_chunks = []\n",
    "    for chunk in chunks:\n",
    "        all_chunks.append(chunk)\n",
    "    combined_df = pd.concat(all_chunks)\n",
    "    if \"HADM_ID\" in combined_df.columns:\n",
    "        combined_df = combined_df[~combined_df.HADM_ID.isna()]\n",
    "        combined_df[\"HADM_ID\"] = combined_df[\"HADM_ID\"].astype(int)\n",
    "    combined_df.to_pickle(op_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in MICROBIOLOGYEVENTS.pkl:\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'CHARTDATE', 'CHARTTIME', 'SPEC_ITEMID', 'SPEC_TYPE_DESC', 'ORG_ITEMID', 'ORG_NAME', 'ISOLATE_NUM', 'AB_ITEMID', 'AB_NAME', 'DILUTION_TEXT', 'DILUTION_COMPARISON', 'DILUTION_VALUE', 'INTERPRETATION']\n",
      "\n",
      "Columns in PRESCRIPTIONS.pkl:\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'STARTDATE', 'ENDDATE', 'DRUG_TYPE', 'DRUG', 'DRUG_NAME_POE', 'DRUG_NAME_GENERIC', 'FORMULARY_DRUG_CD', 'GSN', 'NDC', 'PROD_STRENGTH', 'DOSE_VAL_RX', 'DOSE_UNIT_RX', 'FORM_VAL_DISP', 'FORM_UNIT_DISP', 'ROUTE']\n",
      "\n",
      "Columns in ADMISSIONS.pkl:\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'ADMITTIME', 'DISCHTIME', 'DEATHTIME', 'ADMISSION_TYPE', 'ADMISSION_LOCATION', 'DISCHARGE_LOCATION', 'INSURANCE', 'LANGUAGE', 'RELIGION', 'MARITAL_STATUS', 'ETHNICITY', 'EDREGTIME', 'EDOUTTIME', 'DIAGNOSIS', 'HOSPITAL_EXPIRE_FLAG', 'HAS_CHARTEVENTS_DATA']\n",
      "\n",
      "Columns in INPUTEVENTS_CV.pkl:\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'CHARTTIME', 'ITEMID', 'AMOUNT', 'AMOUNTUOM', 'RATE', 'RATEUOM', 'STORETIME', 'CGID', 'ORDERID', 'LINKORDERID', 'STOPPED', 'NEWBOTTLE', 'ORIGINALAMOUNT', 'ORIGINALAMOUNTUOM', 'ORIGINALROUTE', 'ORIGINALRATE', 'ORIGINALRATEUOM', 'ORIGINALSITE']\n",
      "\n",
      "Columns in PROCEDUREEVENTS_MV.pkl:\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'STARTTIME', 'ENDTIME', 'ITEMID', 'VALUE', 'VALUEUOM', 'LOCATION', 'LOCATIONCATEGORY', 'STORETIME', 'CGID', 'ORDERID', 'LINKORDERID', 'ORDERCATEGORYNAME', 'SECONDARYORDERCATEGORYNAME', 'ORDERCATEGORYDESCRIPTION', 'ISOPENBAG', 'CONTINUEINNEXTDEPT', 'CANCELREASON', 'STATUSDESCRIPTION', 'COMMENTS_EDITEDBY', 'COMMENTS_CANCELEDBY', 'COMMENTS_DATE']\n",
      "\n",
      "Columns in PATIENTS.pkl:\n",
      "['ROW_ID', 'SUBJECT_ID', 'GENDER', 'DOB', 'DOD', 'DOD_HOSP', 'DOD_SSN', 'EXPIRE_FLAG']\n",
      "\n",
      "Columns in OUTPUTEVENTS.pkl:\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'CHARTTIME', 'ITEMID', 'VALUE', 'VALUEUOM', 'STORETIME', 'CGID', 'STOPPED', 'NEWBOTTLE', 'ISERROR']\n",
      "\n",
      "Columns in SERVICES.pkl:\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'TRANSFERTIME', 'PREV_SERVICE', 'CURR_SERVICE']\n",
      "\n",
      "Columns in NOTEEVENTS.pkl:\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'CHARTDATE', 'CHARTTIME', 'STORETIME', 'CATEGORY', 'DESCRIPTION', 'CGID', 'ISERROR', 'TEXT']\n",
      "\n",
      "Columns in PROCEDURES_ICD.pkl:\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'SEQ_NUM', 'ICD9_CODE']\n",
      "\n",
      "Columns in INPUTEVENTS_MV.pkl:\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'STARTTIME', 'ENDTIME', 'ITEMID', 'AMOUNT', 'AMOUNTUOM', 'RATE', 'RATEUOM', 'STORETIME', 'CGID', 'ORDERID', 'LINKORDERID', 'ORDERCATEGORYNAME', 'SECONDARYORDERCATEGORYNAME', 'ORDERCOMPONENTTYPEDESCRIPTION', 'ORDERCATEGORYDESCRIPTION', 'PATIENTWEIGHT', 'TOTALAMOUNT', 'TOTALAMOUNTUOM', 'ISOPENBAG', 'CONTINUEINNEXTDEPT', 'CANCELREASON', 'STATUSDESCRIPTION', 'COMMENTS_EDITEDBY', 'COMMENTS_CANCELEDBY', 'COMMENTS_DATE', 'ORIGINALAMOUNT', 'ORIGINALRATE']\n",
      "\n",
      "Columns in TRANSFERS.pkl:\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'DBSOURCE', 'EVENTTYPE', 'PREV_CAREUNIT', 'CURR_CAREUNIT', 'PREV_WARDID', 'CURR_WARDID', 'INTIME', 'OUTTIME', 'LOS']\n",
      "\n",
      "Columns in LABEVENTS.pkl:\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'ITEMID', 'CHARTTIME', 'VALUE', 'VALUENUM', 'VALUEUOM', 'FLAG']\n",
      "\n",
      "Columns in CAREGIVERS.pkl:\n",
      "['ROW_ID', 'CGID', 'LABEL', 'DESCRIPTION']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display all columns in all the files\n",
    "\n",
    "\n",
    "# Path to the folder containing pickle files\n",
    "folder_path = \"../data\"\n",
    "os.makedirs(folder_path,exist_ok=True)\n",
    "\n",
    "# Iterate through all files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".pkl\"):  # Check if the file is a pickle file\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        try:\n",
    "            # Load the pickle file into a DataFrame\n",
    "            df = pd.read_pickle(file_path)\n",
    "            print(f\"Columns in {file_name}:\")\n",
    "            print(df.columns.tolist())\n",
    "            print()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_name}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
