{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asusaiyah/projects/general_mimics/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "selected_categories = [\"Nursing\", \"Nursing/other\"]\n",
    "# Add parent directory to sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "import importlib\n",
    "import utils.event_extractor  # your module\n",
    "\n",
    "importlib.reload(utils.event_extractor)\n",
    "from utils.event_extractor import EventExtractor  # re-import your class if needed\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def get_quartiles(df, column=\"LOS_DAYS\"):\n",
    "    \"\"\"\n",
    "    Print the Q1, Q2 (median), and Q3 quartiles for the given column in the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing the length-of-stay data.\n",
    "        column (str): Name of the column to compute quartiles on (default: \"LOS_DAYS\").\n",
    "    \"\"\"\n",
    "    q1 = df[column].quantile(0.25)\n",
    "    q2 = df[column].median()\n",
    "    q3 = df[column].quantile(0.75)\n",
    "    mid_df = df[(df[column]>=q1)&(df[column]<=q2)]\n",
    "    num = len(mid_df)\n",
    "    \n",
    "    print(f\"{column} Quartiles :\")\n",
    "    print(f\"Q1 (25th percentile): {q1:.2f}\")\n",
    "    print(f\"Q2 (Median):          {q2:.2f}\")\n",
    "    print(f\"Q3 (75th percentile): {q3:.2f}\")\n",
    "    print(f\"Number of rows in this range q1-q3: {num}\")\n",
    "\n",
    "    return q1,q2,q3,mid_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the admissions data\n",
    "admissions = pd.read_csv(\n",
    "    \"../mimic-iii-clinical-database-1.4/ADMISSIONS.csv\",\n",
    "    parse_dates=[\"ADMITTIME\", \"DISCHTIME\"]\n",
    ")\n",
    "\n",
    "# Calculate Length of Stay in days\n",
    "admissions[\"LOS_DAYS\"] = (admissions[\"DISCHTIME\"] - admissions[\"ADMITTIME\"]).dt.total_seconds() / (24 * 3600)\n",
    "\n",
    "# stay_q1,stay_q2,stay_q3 = get_quartiles(admissions, \"LOS_DAYS\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# # Path to the folder containing pickle files\n",
    "# folder_path = \"../mimic-iii-clinical-database-1.4\"\n",
    "\n",
    "# # Iterate through all files in the folder\n",
    "# for file_name in os.listdir(folder_path):\n",
    "#     if file_name.endswith(\".pkl\"):  # Check if the file is a pickle file\n",
    "#         file_path = os.path.join(folder_path, file_name)\n",
    "#         try:\n",
    "#             # Load the pickle file into a DataFrame\n",
    "#             df = pd.read_pickle(file_path)\n",
    "#             print(f\"Columns in {file_name}:\")\n",
    "#             print(df.columns.tolist())\n",
    "#             print()\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading {file_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valid, alive patients are those who have a valide DOB and do not have a DOD associated to them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patients not known to have died: 30761\n",
      "   ROW_ID  SUBJECT_ID GENDER        DOB DOD DOD_HOSP DOD_SSN  EXPIRE_FLAG\n",
      "0     234         249      F 2075-03-13 NaT      NaT     NaT            0\n",
      "2     236         251      M 2090-03-15 NaT      NaT     NaT            0\n",
      "3     237         252      M 2078-03-06 NaT      NaT     NaT            0\n",
      "4     238         253      F 2089-11-26 NaT      NaT     NaT            0\n",
      "5     239         255      M 2109-08-05 NaT      NaT     NaT            0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timestamp('1800-07-16 00:00:00')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load the PATIENTS.pkl file\n",
    "patients = pd.read_pickle(\"../mimic-iii-clinical-database-1.4/PATIENTS.pkl\")\n",
    "notes = pd.read_pickle(\"../mimic-iii-clinical-database-1.4/NOTEEVENTS.pkl\")\n",
    "notes['HADM_ID'] = notes['HADM_ID'].fillna(0).astype(int)\n",
    "date_cols = ['DOB', 'DOD', 'DOD_HOSP', 'DOD_SSN']\n",
    "patients[date_cols] = patients[date_cols].apply(pd.to_datetime, errors='coerce')\n",
    "\n",
    "# Filter patients with no recorded death\n",
    "alive_patients = patients[\n",
    "    patients['DOD'].isna() &\n",
    "    patients['DOD_HOSP'].isna() &\n",
    "    patients['DOD_SSN'].isna()\n",
    "]\n",
    "\n",
    "print(f\"Number of patients not known to have died: {len(alive_patients)}\")\n",
    "print(alive_patients.head())\n",
    "\n",
    "\n",
    "valid_alive_patients = alive_patients\n",
    "valid_alive_patients['DOB'].min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "def calculate_age(row):\n",
    "    admit = row['ADMITTIME'].to_pydatetime()\n",
    "    dob = row['DOB'].to_pydatetime()\n",
    "    return (admit - dob).days / 365.25\n",
    "\n",
    "subject_id_to_dob = {i:j for (i,j) in zip(valid_alive_patients[\"SUBJECT_ID\"], valid_alive_patients[\"DOB\"])}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "grouped_notes = notes[notes.CATEGORY.isin(selected_categories)].groupby(['HADM_ID']).count()['TEXT'].reset_index().rename(columns={'TEXT': 'COUNT_TEXT'})\n",
    "hadm_to_num_report = {i:j for (i,j) in zip(grouped_notes[\"HADM_ID\"], grouped_notes[\"COUNT_TEXT\"])}\n",
    "\n",
    "\n",
    "\n",
    "admissions['DOB'] = admissions['SUBJECT_ID'].apply(lambda x: subject_id_to_dob.get(x, np.nan))\n",
    "admissions['AGE'] = admissions.apply(calculate_age, axis=1)\n",
    "admissions['NUM_REPORTS'] = admissions['HADM_ID'].apply(lambda x:hadm_to_num_report.get(x, np.nan))\n",
    "admissions = admissions[admissions.AGE<200]\n",
    "\n",
    "\n",
    "admissions.dropna(subset=[\"DOB\",\"NUM_REPORTS\"],inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOS_DAYS Quartiles :\n",
      "Q1 (25th percentile): 3.39\n",
      "Q2 (Median):          5.81\n",
      "Q3 (75th percentile): 10.72\n",
      "Number of rows in this range q1-q3: 6430\n",
      "NUM_REPORTS Quartiles :\n",
      "Q1 (25th percentile): 3.00\n",
      "Q2 (Median):          7.00\n",
      "Q3 (75th percentile): 18.00\n",
      "Number of rows in this range q1-q3: 9068\n",
      "AGE Quartiles :\n",
      "Q1 (25th percentile): 0.00\n",
      "Q2 (Median):          48.07\n",
      "Q3 (75th percentile): 64.72\n",
      "Number of rows in this range q1-q3: 12856\n"
     ]
    }
   ],
   "source": [
    "los_quartiles = get_quartiles(admissions, \"LOS_DAYS\")\n",
    "n_report_quartiles = get_quartiles(admissions, \"NUM_REPORTS\")\n",
    "age_quartiles = get_quartiles(admissions,\"AGE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1332"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_hadm = set(los_quartiles[-1][\"HADM_ID\"].tolist()).intersection(n_report_quartiles[-1][\"HADM_ID\"].tolist()).intersection(age_quartiles[-1][\"HADM_ID\"].to_list())\n",
    "len(common_hadm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50195/373065821.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_reports_df['Sentences'] = filtered_reports_df['TEXT'].apply(extract_sentences)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "\n",
    "# Load the English model (download if you haven't: python -m spacy download en_core_web_sm)\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "known_dict = {}\n",
    "def extract_sentences(text):\n",
    "    global known_dict\n",
    "    if text in known_dict:\n",
    "        return known_dict[text]\n",
    "    else:\n",
    "        doc = nlp(text)\n",
    "        sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "        known_dict[text] = sentences\n",
    "    return sentences \n",
    "\n",
    "filtered_reports_df = notes[(notes[\"HADM_ID\"].isin(common_hadm)) & (notes[\"CATEGORY\"].isin(selected_categories))]\n",
    "filtered_reports_df['Sentences'] = filtered_reports_df['TEXT'].apply(extract_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtered_reports_df.to_pickle(\"../exports/filtered_patient_reports.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminate people admitted younger than 18yo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'exports/filtered_patient_reports.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[176]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m filtered_reports_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexports/filtered_patient_reports.pkl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m extractor = EventExtractor(event_name_model_type=\u001b[33m\"\u001b[39m\u001b[33mbiolord\u001b[39m\u001b[33m\"\u001b[39m, attribute_model_type=\u001b[33m\"\u001b[39m\u001b[33mllama3\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_events\u001b[39m(sentences):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/general_mimics/.venv/lib/python3.12/site-packages/pandas/io/pickle.py:185\u001b[39m, in \u001b[36mread_pickle\u001b[39m\u001b[34m(filepath_or_buffer, compression, storage_options)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[33;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[32m    125\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m \u001b[33;03m4    4    9\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    184\u001b[39m excs_to_catch = (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    192\u001b[39m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[32m    193\u001b[39m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[32m    194\u001b[39m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    197\u001b[39m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[32m    198\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/general_mimics/.venv/lib/python3.12/site-packages/pandas/io/common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'exports/filtered_patient_reports.pkl'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "filtered_reports_df = pd.read_pickle(\"../exports/filtered_patient_reports.pkl\")\n",
    "extractor = EventExtractor(event_name_model_type=\"biolord\", attribute_model_type=\"llama3\")\n",
    "\n",
    "def extract_events(sentences):\n",
    "    global extractor\n",
    "    event_types = [\"Pain\", \"Sleep\"]\n",
    "    events = extractor.extract_events(sentences=sentences, event_names=event_types, threshold=0.2)\n",
    "    return events\n",
    "\n",
    "filtered_reports_df_debug = filtered_reports_df.head(1)\n",
    "filtered_reports_df_debug['Events'] = filtered_reports_df_debug['Sentences'].apply(extract_events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
